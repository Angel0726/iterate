---
title: 大纲
toc: true
date: 2019-08-20
---
# 大纲


模型压缩与 ADMM 有关系吗？

熟悉 pruning、quantization、knowledge distillation等常用的模型压缩方法优先；

二值化网络放在那里？

补充下彩票假设，好像是很牛逼的。


深度学习因其计算复杂度或参数冗余，在一些场景和设备上限制了相应的模型部署，需要借助模型压缩、优化加速、异构计算等方法突破瓶颈。



## 主要内容


## 可以补充进来的


## 需要消化的


- [ICLR2019最佳论文！神经网络子网络压缩 10 倍，精确度还能保持不变](https://zhuanlan.zhihu.com/p/64929909) 彩票假设 好像是很牛逼的，涉及到根本的一些东西了。
- [彩票假设如何挑战我们所知的训练神经网络的一切](https://www.infoq.cn/article/HxYdlYXYSC1O6*sRPJi8)
- [端上智能——深度学习模型压缩与加速](https://www.jianshu.com/p/c34ec77dae9e)

- [当前深度神经网络模型压缩和加速都有哪些方法？](https://www.jiqizhixin.com/articles/2018-05-22-9)
- [AI综述专栏 | 深度神经网络加速与压缩](https://zhuanlan.zhihu.com/p/50938836)
- [模型压缩总览](https://www.jianshu.com/p/e73851f32c9f)
- [深度学习模型压缩与加速综述](https://zhuanlan.zhihu.com/p/67871864)

- [想在手机上用自己的机器学习模型？谷歌爸爸的模型压缩包你满意](https://www.leiphone.com/news/201805/tlOzhASU4RaLU1aF.html)
